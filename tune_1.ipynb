{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "We will tune and pilot with the 500 most frequent monosyllabic words from TASA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-09 14:58:23.486300: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-08-09 14:58:23.494908: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-09 14:58:23.504724: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-09 14:58:23.507630: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-08-09 14:58:23.515193: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-09 14:58:24.137368: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# p39 (used CPU implementation)\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "import time\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "\n",
    "from src.learner import *\n",
    "from utilities import *\n",
    "\n",
    "# data\n",
    "kidwords = pd.read_csv('data/kidwords/kidwords.csv', header=None)[0].tolist()\n",
    "\n",
    "top_500 = pd.read_csv('data/top_500.csv')\n",
    "test_word_indices = np.array([i for i, e in enumerate(kidwords) if e in top_500])\n",
    "\n",
    "words, X, Y = subset_kidwords(top_500.word.tolist(), kidwords, np.genfromtxt('data/kidwords/orth.csv', delimiter=\",\"), np.genfromtxt('data/kidwords/phon.csv', delimiter=\",\"), remove_null_columns=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPU check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1723233508.564497   39425 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1723233508.585530   39425 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1723233508.585656   39425 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 323\n",
    "\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "with open('outputs/tune_top_500_v1.csv', 'w') as f:\n",
    "    f.write(\"{},{},{},{},{},{},{},{},{},{},{}\\n\".format(\n",
    "                                            \"hidden_units\",\n",
    "                                            \"learning_rate\",\n",
    "                                             \"batch_size\",\n",
    "                                             \"epochs\",\n",
    "                                             \"loss_train\",\n",
    "                                             \"accuracy_train\",\n",
    "                                             \"mse_train\",\n",
    "                                             \"loss_test\",\n",
    "                                             \"accuracy_test\",\n",
    "                                             \"mse_test\",\n",
    "                                             \"time\"))\n",
    "    for learning_rate in [.001, .005, .01, .025, None]: \n",
    "        for batch_size in [10, 20, 30, 40, 50]:\n",
    "            for epochs in [20, 40, 60]:\n",
    "                for hidden in [8, 12, 16, 20]:\n",
    "                    \n",
    "                    print(\"Configuration currently training:\", learning_rate, batch_size, epochs, hidden)\n",
    "\n",
    "                    if learning_rate is not None:\n",
    "                        optimizer = Adam(learning_rate=learning_rate)\n",
    "                    if learning_rate is None:\n",
    "                        optimzer = None\n",
    "\n",
    "                    model = learner(X, Y, seed, hidden, optimizer=None)\n",
    "                    \n",
    "                    start_time = time.time()\n",
    "\n",
    "                    model.fit(X, Y, epochs=epochs, batch_size=batch_size, verbose=True)\n",
    "\n",
    "                    end_time = time.time()\n",
    "                    runtime = end_time - start_time\n",
    "\n",
    "                    loss_train, accuracy_train, mse_train = model.evaluate(X, Y, verbose=0) \n",
    "\n",
    "                    f.write(\"{},{},{},{},{},{},{},{}\\n\".format(\n",
    "                                                    hidden,\n",
    "                                                    learning_rate,\n",
    "                                                    batch_size,\n",
    "                                                    epochs,\n",
    "                                                    loss_train,\n",
    "                                                    accuracy_train,\n",
    "                                                    mse_train,\n",
    "                                                    runtime))\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
